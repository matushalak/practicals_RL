{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d0e355",
   "metadata": {},
   "source": [
    "# Reinforcement learning practical sessions - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496fe3e",
   "metadata": {},
   "source": [
    "## Workshop tutorial, day 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086ba19c",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning Agent (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a72ffd",
   "metadata": {},
   "source": [
    "### Author: Buelent Uendes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd5edf",
   "metadata": {},
   "source": [
    "Before we dive into deep reinforcement learning, it is a good idea to familiarize ourselves with the gym environment provided by OpenAI and start with the simple Q learning agent. In the second notebook (Deep Reinforcement Learning Agent (Part 2), we will show how we implement a deep reinforcement learning algorithm to solve the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d174629f",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "In the notebook, you will see a couple of ToDos with some instructions. Try your best to work through them and to complete the notebook. In case you run into problems, do not hesitate to ask any of the TAs for help! :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d51520",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3833d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "#Set the seed for reproducibility\n",
    "np.random.seed(7)\n",
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc733553",
   "metadata": {},
   "source": [
    "## Open AI gym and the mountain car problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96559ce9",
   "metadata": {},
   "source": [
    "In the following, we will train our reinforcement learning algorithms on classical control problems provided by OpenAI gym. In particular, we will focus on the so-called mountain car problem. In this problem, the agent needs to learn how to drive a car up the right hill, given the constraint that the engine of the car is not strong enough to do so right away. Starting from the bottom of the hill, the agent therefore needs to go back and forth, thereby creating enough momentum, to drive up the mountain. Compared to other classical control problems, this is a quite challenging environment, as the agent never receives a non-negative reward unless the agent reaches the right hill. Thus, most of the time the agent receives a negative reward of -1 which makes learning difficult.\n",
    "\n",
    "You can find more information about the mountain car environment [here](https://www.gymlibrary.ml/environments/classic_control/mountain_car/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effbb09",
   "metadata": {},
   "source": [
    "## Quick recap of working with the gym environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d09954c",
   "metadata": {},
   "source": [
    "## 1) Creating the environment \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912be3db",
   "metadata": {},
   "source": [
    "Creating an environment, is fairly easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a56681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "#Set the seed\n",
    "env.action_space.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f11286",
   "metadata": {},
   "source": [
    "## 2) Getting familiar with some of the properties of the problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc883cd",
   "metadata": {},
   "source": [
    "Before starting to work on the problem, it is important to familiarize oneself with the problem at hand, i.e. investigating the observation space and the action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b55f945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The observation space is  Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "The action space is  Discrete(3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('The observation space is ', env.observation_space)\n",
    "print('The action space is ', env.action_space)\n",
    "\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d7ff7",
   "metadata": {},
   "source": [
    "As we can see, the observation space is continous with discrete number of actions. The agent can choose from three different actions:\n",
    "\n",
    "- 0: Accelerate to the left\n",
    "- 1: Do not accelerate\n",
    "- 2: Accelerate to the right\n",
    "\n",
    "One way to deal with continous action space is to discretize the state space into bins. This simplifies the problem and is a prerequisite for any tabular approach such as the Q learning.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea4d93",
   "metadata": {},
   "source": [
    "To get a better idea of the observation space, we can check the range of the observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "low = env.observation_space.low\n",
    "high =env.observation_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5391a23",
   "metadata": {},
   "source": [
    "The first value of the observation space represents the x position, whereas the second one represents the velocity of the car. As we can see, the x position is in the range between $[-1.2, 0.6]$, whereas velocity lies in the range between $[-0.07, 0.07]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122dfa9",
   "metadata": {},
   "source": [
    "To create a new starting position, one can use the reset command as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807904d",
   "metadata": {},
   "source": [
    "We an sample a random action from the environment via the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21fecc",
   "metadata": {},
   "source": [
    "## First step: Random agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca638e67",
   "metadata": {},
   "source": [
    "It is always a good idea to test out the environment and start to play around. For this purpose, one can create an agent that plays the game with random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(env_name, render_mode='human')\n",
    "eval_env.reset()\n",
    "for _ in range(500):\n",
    "    if _ % 100 == 0:\n",
    "        print(_)\n",
    "    action = eval_env.step(env.action_space.sample())\n",
    "\n",
    "# This will stop the environment after\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ecd9c",
   "metadata": {},
   "source": [
    "## In conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b9403",
   "metadata": {},
   "source": [
    "As a recap, the main four functions for the environment space are:\n",
    "- **env.reset():** \n",
    "    Resets the environment and obtain initial starting observation\n",
    "- **env.render():** \n",
    "    Visualize the environment. Important Pygame needs to be installed for this\n",
    "- **env.step(action):** \n",
    "    Applies an action to it. It outputes next state, reward, done and info\n",
    "- **env.close():** \n",
    "    Closes the pop-up frame of the visualized environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2427cbbf",
   "metadata": {},
   "source": [
    "## Main problem: Let's learn an agent that uses Q-learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcade68a",
   "metadata": {},
   "source": [
    "As mentioned above, we will first use implement a classical approach of TD-learning, i.e. Q-learning. Q-learning is an off-policy, tabular approach to reinforcement learning. Hence, we need to discretize the state space before proceeding.\n",
    "\n",
    "It is important to keep in mind that we do not want to discretize the state space too much, given the **GLIE theorem**. **GLIE theorem stands for 'Greedy in the Limit with Infinite Exploration', which means that the state action pairs Q(s,a) will converge to the optimal ones if all state-action pairs are explored infinitely many times and the policy converges to the greedy one**. For this reason, we want to have not too many states and we need to tune the epsilon rate in such a way that it converges to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66093a5f",
   "metadata": {},
   "source": [
    "**ToDo:**\n",
    "\n",
    "In the following QAgent class, there are specific code sections missing. Please complete these so you can finalize the QAgent. If you get stuck, do not hesitate to ask the TAs!\n",
    "\n",
    "In particular, you will be asked to implement parts of the discretizing, initializing the Q matrix and the epsilon greedy policy. These missing parts are all highlighted by a TODO!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92768c6d",
   "metadata": {},
   "source": [
    "## Finalizing the Q learning agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16655903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    \n",
    "    def __init__(self, env_name, discount_rate = 0.95, bin_size = 20):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        \n",
    "        env_name = name of the specific environment that the agent wants to solve\n",
    "        discount_rate = discount rate used for future rewards\n",
    "        bin_size = number of bins used for discretizing the state space\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #create an environment\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        \n",
    "        #Set the discount rate\n",
    "        self.discount_rate = discount_rate\n",
    "        \n",
    "        #The algorithm has then 3 different actions\n",
    "        #0: Accelerate to the left\n",
    "        #1: Don't accelerate\n",
    "        #2: Accelerate to the right\n",
    "        self.action_space = self.env.action_space.n\n",
    "        \n",
    "        #Set the bin size\n",
    "        self.bin_size = bin_size\n",
    "        \n",
    "        #State incorporates the observation state\n",
    "        #State[0] is x position\n",
    "        #State[1] is velocity\n",
    "    \n",
    "        #Get the low and high values of the environment space\n",
    "        self.low = self.env.observation_space.low\n",
    "        self.high = self.env.observation_space.high\n",
    "    \n",
    "        #Create bins for both observation features, i.e. x-position and velocity\n",
    "        \n",
    "        self.bin_x = np.linspace(self.low[0], self.high[0], self.bin_size)\n",
    "    \n",
    "        '''\n",
    "        ToDo:\n",
    "        \n",
    "        Please create the bins for the velocity feature in the same manner and call this variable self.bin_velocity!\n",
    "        '''\n",
    "                \n",
    "        #Solution\n",
    "        self.bin_velocity = np.linspace(self.low[1], self.high[1], self.bin_size)\n",
    "        \n",
    "        #Append the two bins\n",
    "        self.bins = [self.bin_x, self.bin_velocity]\n",
    "    \n",
    "    def discretize_state(self, state):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        state = state observation that needs to be discretized\n",
    "        \n",
    "        \n",
    "        Returns:\n",
    "        discretized state\n",
    "        '''\n",
    "        #Now we can make use of the function np.digitize and bin it\n",
    "        self.state = state\n",
    "        \n",
    "        #Create an empty state\n",
    "        digitized_state = []\n",
    "    \n",
    "        for i in range(len(self.bins)):\n",
    "            digitized_state.append(np.digitize(self.state[i], self.bins[i])-1)\n",
    "        \n",
    "        #Returns the discretized state from an observation\n",
    "        return digitized_state\n",
    "    \n",
    "    def create_Q_table(self):\n",
    "        self.state_space = self.bin_size - 1\n",
    "        #Initialize all values in the Q-table to zero\n",
    "        \n",
    "        '''\n",
    "        ToDo:\n",
    "        Initialize a zero matrix of dimension state_space * state_space * action_space and call it self.Qtable!\n",
    "        '''\n",
    "        \n",
    "        #Solution:\n",
    "        self.Qtable = np.zeros((self.state_space, self.state_space, self.action_space))\n",
    "        \n",
    "\n",
    "    def train(self, simulations, learning_rate, epsilon = 0.05, epsilon_decay = 1000, adaptive_epsilon = False, \n",
    "              adapting_learning_rate = False):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        \n",
    "        simulations = number of episodes of a game to run\n",
    "        learning_rate = learning rate for the update equation\n",
    "        epsilon = epsilon value for epsilon-greedy algorithm\n",
    "        epsilon_decay = number of full episodes (games) over which the epsilon value will decay to its final value\n",
    "        adaptive_epsilon = boolean that indicates if the epsilon rate will decay over time or not\n",
    "        adapting_learning_rate = boolean that indicates if the learning rate should be adaptive or not\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Initialize variables that keep track of the rewards\n",
    "        \n",
    "        self.rewards = []\n",
    "        self.average_rewards = []\n",
    "        \n",
    "        #Call the Q table function to create an initialized Q table\n",
    "        self.create_Q_table()\n",
    "        \n",
    "        #Set epsilon rate, epsilon decay and learning rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        #Set start epsilon, so here we want a starting exploration rate of 1\n",
    "        self.epsilon_start = 1\n",
    "        self.epsilon_end = 0.05\n",
    "        \n",
    "        #If we choose adaptive learning rate, we start with a value of 1 and decay it over time!\n",
    "        if adapting_learning_rate:\n",
    "            self.learning_rate = 1\n",
    "        \n",
    "        for i in range(simulations):\n",
    "            \n",
    "            if i % 5000 == 0:\n",
    "                print(f'Please wait, the algorithm is learning! The current simulation is {i}')\n",
    "            #Initialize the state\n",
    "            state = self.env.reset()[0]   # reset returns a dict, need to take the 0th entry.\n",
    "        \n",
    "            #Set a variable that flags if an episode has terminated\n",
    "            done = False\n",
    "        \n",
    "            #Discretize the state space\n",
    "            \n",
    "            state = self.discretize_state(state)\n",
    "            \n",
    "            #Set the rewards to 0\n",
    "            total_rewards = 0\n",
    "            \n",
    "            #If adaptive epsilon rate\n",
    "            if adaptive_epsilon:\n",
    "                self.epsilon = np.interp(i, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n",
    "                \n",
    "                #Logging just to check it decays as we want it to do, we just print out the first three statements\n",
    "                if i % 500 == 0 and i <= 1500:\n",
    "                    print(f\"The current epsilon rate is {self.epsilon}\")\n",
    "                \n",
    "            #Loop until an episode has terminated\n",
    "            while not done:\n",
    "                \n",
    "                #Pick an action based on epsilon greedy\n",
    "                \n",
    "                '''\n",
    "                ToDo: Write the if statement that picks a random action\n",
    "                Tip: Make use of np.random.uniform() and the self.epsilon to make a decision!\n",
    "                Tip: You can also make use of the method sample() of the self.env.action_space \n",
    "                    to generate a random action!\n",
    "                '''\n",
    "                \n",
    "                #Solution:\n",
    "                \n",
    "                #Pick random action\n",
    "                if np.random.uniform(0,1) > 1-self.epsilon:\n",
    "                    #This picks a random action from 0,1,2\n",
    "                    action = self.env.action_space.sample()\n",
    "                    \n",
    "                    \n",
    "                #Pick a greedy action\n",
    "                else:\n",
    "                    action = np.argmax(self.Qtable[state[0],state[1],:])\n",
    "                    \n",
    "                #Now sample the next_state, reward, done and info from the environment\n",
    "                \n",
    "                next_state, reward, terminated, truncated, info = self.env.step(action) # step returns 5 outputs\n",
    "                done =  terminated or truncated\n",
    "                \n",
    "                #Now discretize the next_state\n",
    "                next_state = self.discretize_state(next_state)\n",
    "                \n",
    "                #Target value \n",
    "                Q_target = (reward + self.discount_rate*np.max(self.Qtable[next_state[0], next_state[1]]))\n",
    "                \n",
    "                #Calculate the Temporal difference error (delta)\n",
    "                delta = self.learning_rate * (Q_target - self.Qtable[state[0], state[1], action])\n",
    "                \n",
    "                #Update the Q-value\n",
    "                self.Qtable[state[0], state[1], action] = self.Qtable[state[0], state[1], action] + delta\n",
    "                \n",
    "                #Update the reward and the hyperparameters\n",
    "                total_rewards += reward\n",
    "                state = next_state\n",
    "                \n",
    "            \n",
    "            if adapting_learning_rate:\n",
    "                self.learning_rate = self.learning_rate/np.sqrt(i+1)\n",
    "            \n",
    "            self.rewards.append(total_rewards)\n",
    "            \n",
    "            #Calculate the average score over 100 episodes\n",
    "            if i % 100 == 0:\n",
    "                self.average_rewards.append(np.mean(self.rewards))\n",
    "                \n",
    "                #Initialize a new reward list, as otherwise the average values would reflect all rewards!\n",
    "                self.rewards = []\n",
    "        \n",
    "        print('The simulation is done!')\n",
    "        \n",
    "    def visualize_rewards(self):\n",
    "        plt.figure(figsize =(7.5,7.5))\n",
    "        plt.plot(100*(np.arange(len(self.average_rewards))+1), self.average_rewards)\n",
    "        plt.axhline(y = -110, color = 'r', linestyle = '-')\n",
    "        plt.title('Average reward over the past 100 simulations', fontsize = 10)\n",
    "        plt.legend(['Q-learning performance','Benchmark'])\n",
    "        plt.xlabel('Number of simulations', fontsize = 10)\n",
    "        plt.ylabel('Average reward', fontsize = 10)\n",
    "            \n",
    "    def play_game(self):\n",
    "        # Make eval env which renders when taking a step\n",
    "        eval_env = gym.make(env_name, render_mode='human')\n",
    "        state = eval_env.reset()[0]\n",
    "        done=False\n",
    "        # Run the environment for 1 episode\n",
    "        while not done:\n",
    "            state = self.discretize_state(state)\n",
    "            action = np.argmax(self.Qtable[state[0],state[1],:])\n",
    "            next_state, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "        eval_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3223f42b",
   "metadata": {},
   "source": [
    "## Let's run this and train the QAgent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#State the name of the game\n",
    "env_name = 'MountainCar-v0'\n",
    "\n",
    "#You can change the learning rate and the number of simulations if you want (yet, it will take then of course longer)!\n",
    "simulations = 15000\n",
    "learning_rate = 0.10\n",
    "\n",
    "'''\n",
    "ToDo:\n",
    "Initialize the Qagent and call the instance agent!\n",
    "Afterwards,let it train over the number of simulations with the specific learning rate!\n",
    "'''\n",
    "\n",
    "#WRITE YOUR CODE HERE!\n",
    "#With standard epsilon_greedy\n",
    "agent_standard_greedy = QAgent(env_name)\n",
    "agent_standard_greedy.train(simulations, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also train the Qagent with a decaying epsilon schedule\n",
    "\n",
    "agent_epsilon_decay = QAgent(env_name)\n",
    "agent_epsilon_decay.train(simulations, learning_rate, adaptive_epsilon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27573627",
   "metadata": {},
   "source": [
    "## Now we can plot the results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b08347",
   "metadata": {},
   "source": [
    "First, we can plot the results for the Qagent that uses the standard epsilon greedy approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0838f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_standard_greedy.visualize_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed8d992",
   "metadata": {},
   "source": [
    "Next, we can see how the Qagent performs that uses an epsilon decaying schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_epsilon_decay.visualize_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca797351",
   "metadata": {},
   "source": [
    "Even though the TD algorithm improves over time, it fails to completely solve the environment, as it does not reach the benchmark of reaching an average reward of -110 ([check this link for further info](https://github.com/openai/gym/wiki/Leaderboard#mountaincar-v0)). Yet, we can check if the algorithm at least manages to the drive up the hill (see next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0acf56",
   "metadata": {},
   "source": [
    "## Lastly, and finally we want to see if our QAgent learned the problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc0e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_epsilon_decay.play_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0707f32",
   "metadata": {},
   "source": [
    "**Done! Did your agent managed to climb up the hill? ;)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2522147",
   "metadata": {},
   "source": [
    "## Further extensions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba9375d",
   "metadata": {},
   "source": [
    "The code above involves the standard tabular Q learning approach. Yet, one can extend this framework, by for example, \n",
    "make a warm start that, i.e. set the starting position different from the bottom of the hill, to help the agent explore the state space better. Yet, one can consider this as a form of cheating. \n",
    "\n",
    "Another possible extension is to do some form of reward shaping. Yet, this requires domain knowledge of the problem. For instance, in this case, we could change the reward function in a way that it also rewards velocity, as this is required for climbing up the hill. However, reward shaping is not a straightforward task, as one might introduce a wrong incentive for the agent which might counteract the initial goal!\n",
    "\n",
    "**In the second notebook, we will see how we can extend the framework by approximating the Q function with a neural network, i.e. by introducing deep reinforcement learning!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
